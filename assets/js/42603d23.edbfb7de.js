"use strict";(self.webpackChunkpathling_site=self.webpackChunkpathling_site||[]).push([[287],{3905:(e,a,t)=>{t.d(a,{Zo:()=>u,kt:()=>g});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),c=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},u=function(e){var a=c(e.components);return n.createElement(s.Provider,{value:a},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},f=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=c(t),f=r,g=p["".concat(s,".").concat(f)]||p[f]||d[f]||o;return t?n.createElement(g,l(l({ref:a},u),{},{components:t})):n.createElement(g,l({ref:a},u))}));function g(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=f;var i={};for(var s in a)hasOwnProperty.call(a,s)&&(i[s]=a[s]);i.originalType=e,i[p]="string"==typeof e?e:r,l[1]=i;for(var c=2;c<o;c++)l[c]=t[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}f.displayName="MDXCreateElement"},5162:(e,a,t)=>{t.d(a,{Z:()=>l});var n=t(7294),r=t(6010);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:a,hidden:t,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,l),hidden:t},a)}},4866:(e,a,t)=>{t.d(a,{Z:()=>S});var n=t(7462),r=t(7294),o=t(6010),l=t(2466),i=t(6550),s=t(1980),c=t(7392),u=t(12);function p(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:t,attributes:n,default:r}}=e;return{value:a,label:t,attributes:n,default:r}}))}function d(e){const{values:a,children:t}=e;return(0,r.useMemo)((()=>{const e=a??p(t);return function(e){const a=(0,c.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,t])}function f(e){let{value:a,tabValues:t}=e;return t.some((e=>e.value===a))}function g(e){let{queryString:a=!1,groupId:t}=e;const n=(0,i.k6)(),o=function(e){let{queryString:a=!1,groupId:t}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:a,groupId:t});return[(0,s._X)(o),(0,r.useCallback)((e=>{if(!o)return;const a=new URLSearchParams(n.location.search);a.set(o,e),n.replace({...n.location,search:a.toString()})}),[o,n])]}function m(e){const{defaultValue:a,queryString:t=!1,groupId:n}=e,o=d(e),[l,i]=(0,r.useState)((()=>function(e){let{defaultValue:a,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!f({value:a,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const n=t.find((e=>e.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:a,tabValues:o}))),[s,c]=g({queryString:t,groupId:n}),[p,m]=function(e){let{groupId:a}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(a),[n,o]=(0,u.Nk)(t);return[n,(0,r.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:n}),k=(()=>{const e=s??p;return f({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{k&&i(k)}),[k]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!f({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),m(e)}),[c,m,o]),tabValues:o}}var k=t(2389);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function h(e){let{className:a,block:t,selectedValue:i,selectValue:s,tabValues:c}=e;const u=[],{blockElementScrollPositionUntilNextRender:p}=(0,l.o5)(),d=e=>{const a=e.currentTarget,t=u.indexOf(a),n=c[t].value;n!==i&&(p(a),s(n))},f=e=>{let a=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=u.indexOf(e.currentTarget)+1;a=u[t]??u[0];break}case"ArrowLeft":{const t=u.indexOf(e.currentTarget)-1;a=u[t]??u[u.length-1];break}}a?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},a)},c.map((e=>{let{value:a,label:t,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:i===a?0:-1,"aria-selected":i===a,key:a,ref:e=>u.push(e),onKeyDown:f,onClick:d},l,{className:(0,o.Z)("tabs__item",b.tabItem,l?.className,{"tabs__item--active":i===a})}),t??a)})))}function y(e){let{lazy:a,children:t,selectedValue:n}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(a){const e=o.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,a)=>(0,r.cloneElement)(e,{key:a,hidden:e.props.value!==n}))))}function v(e){const a=m(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",b.tabList)},r.createElement(h,(0,n.Z)({},e,a)),r.createElement(y,(0,n.Z)({},e,a)))}function S(e){const a=(0,k.Z)();return r.createElement(v,(0,n.Z)({key:String(a)},e))}},3559:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>u,contentTitle:()=>s,default:()=>g,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var n=t(7462),r=(t(7294),t(3905)),o=t(4866),l=t(5162);const i={sidebar_position:4},s="Spark configuration",c={unversionedId:"libraries/installation/spark",id:"libraries/installation/spark",title:"Spark configuration",description:"Session configuration",source:"@site/docs/libraries/installation/spark.md",sourceDirName:"libraries/installation",slug:"/libraries/installation/spark",permalink:"/docs/libraries/installation/spark",draft:!1,editUrl:"https://github.com/aehrc/pathling/tree/main/site/docs/libraries/installation/spark.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"libraries",previous:{title:"Databricks installation",permalink:"/docs/libraries/installation/databricks"},next:{title:"FHIR encoders",permalink:"/docs/libraries/encoders"}},u={},p=[{value:"Session configuration",id:"session-configuration",level:2},{value:"Cluster configuration",id:"cluster-configuration",level:2}],d={toc:p},f="wrapper";function g(e){let{components:a,...t}=e;return(0,r.kt)(f,(0,n.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"spark-configuration"},"Spark configuration"),(0,r.kt)("h2",{id:"session-configuration"},"Session configuration"),(0,r.kt)("p",null,"When you create a ",(0,r.kt)("inlineCode",{parentName:"p"},"PathlingContext")," within your Spark application, it will\ndetect the presence of an existing ",(0,r.kt)("inlineCode",{parentName:"p"},"SparkSession")," and use it. If there is no\nexisting session, it will create one for you with some sensible default\nconfiguration. You can override this default configuration by passing\na ",(0,r.kt)("inlineCode",{parentName:"p"},"SparkSession")," object to the ",(0,r.kt)("inlineCode",{parentName:"p"},"PathlingContext")," constructor."),(0,r.kt)("p",null,"This can be useful if you want to set other Spark configuration, for example to\nincrease the available memory."),(0,r.kt)("p",null,"The session that you provide must have the Pathling library API on the\nclasspath. You can also optionally enable ",(0,r.kt)("a",{parentName:"p",href:"https://delta.io/"},"Delta Lake"),"\nsupport. Here is an example of how to programmatically configure a session that\nhas Delta enabled:"),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from pathling import PathlingContext\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.config(\n            "spark.jars.packages",\n            "au.csiro.pathling:library-runtime:6.4.2,"\n            "io.delta:delta-core_2.12:2.4.0,"\n    )\n    .config(\n            "spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension"\n    )\n    .config(\n            "spark.sql.catalog.spark_catalog",\n            "org.apache.spark.sql.delta.catalog.DeltaCatalog",\n    )\n)\n\npc = PathlingContext.create(spark)\n'))),(0,r.kt)(l.Z,{value:"r",label:"R",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-r"},'library(sparklyr)\nlibrary(pathling)\n\nsc <- spark_connect(master = "local",\n                    packages = c(paste("au.csiro.pathling:library-runtime:", pathling_version()), \n                                 "io.delta:delta-core_2.12:2.4.0"),\n                    config = list("sparklyr.shell.conf" = c(\n                      "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension",\n                      "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"\n                    )), version = "3.4.0")\n\npc <- pathling_connect(sc)\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import au.csiro.pathling.library.PathlingContext\n\nval spark = SparkSession.builder\n  .config("spark.jars.packages", "au.csiro.pathling:library-runtime:6.4.2," +\n      "io.delta:delta-core_2.12:2.4.0")\n  .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")\n  .config("spark.sql.catalog.spark_catalog",\n    "org.apache.spark.sql.delta.catalog.DeltaCatalog")\n  .getOrCreate()\n\nval pc = PathlingContext.create(spark)\n'))),(0,r.kt)(l.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import au.csiro.pathling.library.PathlingContext;\nimport org.apache.spark.sql.SparkSession;\n\nclass MyApp {\n\n    public static void main(String[] args) {\n        SparkSession spark = SparkSession.builder()\n            .config("spark.jars.packages", \n                    "au.csiro.pathling:library-runtime:6.4.2," +\n                    "io.delta:delta-core_2.12:2.4.0")\n            .config("spark.sql.extensions", \n                    "io.delta.sql.DeltaSparkSessionExtension")\n            .config("spark.sql.catalog.spark_catalog",\n                    "org.apache.spark.sql.delta.catalog.DeltaCatalog")\n            .getOrCreate();\n        PathlingContext pc = PathlingContext.create(spark);\n    }\n}\n')))),(0,r.kt)("h2",{id:"cluster-configuration"},"Cluster configuration"),(0,r.kt)("p",null,"If you are running your own Spark cluster, or using a Docker image (such\nas ",(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/jupyter/all-spark-notebook"},"jupyter/all-spark-notebook"),"),\nyou will need to configure Pathling as a Spark package."),(0,r.kt)("p",null,"You can do this by adding the following to your ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"spark.jars.packages au.csiro.pathling:library-runtime:[some version]\n")),(0,r.kt)("p",null,"See the ",(0,r.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/configuration.html"},"Configuration"),"\npage of the Spark documentation for more information about ",(0,r.kt)("inlineCode",{parentName:"p"},"spark.jars.packages"),"\nand other related configuration options."),(0,r.kt)("p",null,"To create a Pathling notebook Docker image, your ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," might look like\nthis:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-dockerfile"},'FROM jupyter/all-spark-notebook\n\nUSER root\nRUN echo "spark.jars.packages au.csiro.pathling:library-runtime:[some version]" >> /usr/local/spark/conf/spark-defaults.conf\n\nUSER ${NB_UID}\n\nRUN pip install --quiet --no-cache-dir pathling && \\\n    fix-permissions "${CONDA_DIR}" && \\\n    fix-permissions "/home/${NB_USER}"\n')))}g.isMDXComponent=!0}}]);