"use strict";(self.webpackChunkpathling_site=self.webpackChunkpathling_site||[]).push([[287],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>g});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),p=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},c=function(e){var a=p(e.components);return n.createElement(s.Provider,{value:a},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=p(t),m=r,g=u["".concat(s,".").concat(m)]||u[m]||d[m]||o;return t?n.createElement(g,l(l({ref:a},c),{},{components:t})):n.createElement(g,l({ref:a},c))}));function g(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=m;var i={};for(var s in a)hasOwnProperty.call(a,s)&&(i[s]=a[s]);i.originalType=e,i[u]="string"==typeof e?e:r,l[1]=i;for(var p=2;p<o;p++)l[p]=t[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},5162:(e,a,t)=>{t.d(a,{Z:()=>l});var n=t(7294),r=t(6010);const o="tabItem_Ymn6";function l(e){let{children:a,hidden:t,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o,l),hidden:t},a)}},5488:(e,a,t)=>{t.d(a,{Z:()=>m});var n=t(7462),r=t(7294),o=t(6010),l=t(2389),i=t(7392),s=t(7094),p=t(2466);const c="tabList__CuJ",u="tabItem_LNqP";function d(e){const{lazy:a,block:t,defaultValue:l,values:d,groupId:m,className:g}=e,f=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),h=d??f.map((e=>{let{props:{value:a,label:t,attributes:n}}=e;return{value:a,label:t,attributes:n}})),k=(0,i.l)(h,((e,a)=>e.value===a.value));if(k.length>0)throw new Error(`Docusaurus error: Duplicate values "${k.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const b=null===l?l:l??f.find((e=>e.props.default))?.props.value??f[0].props.value;if(null!==b&&!h.some((e=>e.value===b)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${b}" but none of its children has the corresponding value. Available values are: ${h.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:y,setTabGroupChoices:v}=(0,s.U)(),[S,E]=(0,r.useState)(b),x=[],{blockElementScrollPositionUntilNextRender:N}=(0,p.o5)();if(null!=m){const e=y[m];null!=e&&e!==S&&h.some((a=>a.value===e))&&E(e)}const C=e=>{const a=e.currentTarget,t=x.indexOf(a),n=h[t].value;n!==S&&(N(a),E(n),null!=m&&v(m,String(n)))},w=e=>{let a=null;switch(e.key){case"Enter":C(e);break;case"ArrowRight":{const t=x.indexOf(e.currentTarget)+1;a=x[t]??x[0];break}case"ArrowLeft":{const t=x.indexOf(e.currentTarget)-1;a=x[t]??x[x.length-1];break}}a?.focus()};return r.createElement("div",{className:(0,o.Z)("tabs-container",c)},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},g)},h.map((e=>{let{value:a,label:t,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:S===a?0:-1,"aria-selected":S===a,key:a,ref:e=>x.push(e),onKeyDown:w,onClick:C},l,{className:(0,o.Z)("tabs__item",u,l?.className,{"tabs__item--active":S===a})}),t??a)}))),a?(0,r.cloneElement)(f.filter((e=>e.props.value===S))[0],{className:"margin-top--md"}):r.createElement("div",{className:"margin-top--md"},f.map(((e,a)=>(0,r.cloneElement)(e,{key:a,hidden:e.props.value!==S})))))}function m(e){const a=(0,l.Z)();return r.createElement(d,(0,n.Z)({key:String(a)},e))}},1316:(e,a,t)=>{t.d(a,{_6:()=>l,fB:()=>r,t8:()=>o});var n=t(7294);const r=()=>n.createElement("p",null,"To use the Pathling encoders from Python, install the"," ",n.createElement("a",{href:"https://pypi.org/project/pathling/"},"pathling")," package using"," ",n.createElement("a",{href:"https://pip.pypa.io/"},"pip"),". Note that Java 11 is required, with your"," ",n.createElement("code",null,"JAVA_HOME")," properly set."),o=()=>n.createElement("p",null,"To use the Pathling encoders from Scala, install the ",n.createElement("code",null,"au.csiro.pathling:encoders")," ",n.createElement("a",{href:"https://maven.apache.org/"},"Maven")," package."),l=()=>n.createElement("p",null,"To use the Pathling encoders from Java, install the ",n.createElement("code",null,"au.csiro.pathling:encoders")," ",n.createElement("a",{href:"https://maven.apache.org/"},"Maven")," package.")},3559:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>p,toc:()=>u});var n=t(7462),r=(t(7294),t(3905)),o=t(5488),l=t(5162);t(1316);const i={sidebar_position:4},s="Spark configuration",p={unversionedId:"libraries/installation/spark",id:"libraries/installation/spark",title:"Spark configuration",description:"Session configuration",source:"@site/docs/libraries/installation/spark.md",sourceDirName:"libraries/installation",slug:"/libraries/installation/spark",permalink:"/docs/libraries/installation/spark",draft:!1,editUrl:"https://github.com/aehrc/pathling/tree/main/site/docs/libraries/installation/spark.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"libraries",previous:{title:"Databricks installation",permalink:"/docs/libraries/installation/databricks"},next:{title:"FHIR encoders",permalink:"/docs/libraries/encoders"}},c={},u=[{value:"Session configuration",id:"session-configuration",level:2},{value:"Cluster configuration",id:"cluster-configuration",level:2}],d={toc:u};function m(e){let{components:a,...t}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"spark-configuration"},"Spark configuration"),(0,r.kt)("h2",{id:"session-configuration"},"Session configuration"),(0,r.kt)("p",null,"When you create a ",(0,r.kt)("inlineCode",{parentName:"p"},"PathlingContext")," within your Spark application, it will\ndetect the presence of an existing ",(0,r.kt)("inlineCode",{parentName:"p"},"SparkSession")," and use it. If there is no\nexisting session, it will create one for you with some sensible default\nconfiguration. You can override this default configuration by passing\na ",(0,r.kt)("inlineCode",{parentName:"p"},"SparkSession")," object to the ",(0,r.kt)("inlineCode",{parentName:"p"},"PathlingContext")," constructor."),(0,r.kt)("p",null,"This can be useful if you want to set other Spark configuration, for example to\nincrease the available memory."),(0,r.kt)("p",null,"The session that you provide must have the Pathling library API on the\nclasspath. You can also optionally enable ",(0,r.kt)("a",{parentName:"p",href:"https://delta.io/"},"Delta Lake"),"\nsupport. Here is an example of how to programmatically configure a session that\nhas Delta enabled:"),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from pathling import PathlingContext, find_jar\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n    .config("spark.jars", find_jar())\n    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.2.0")\n    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")\n    .config("spark.sql.catalog.spark_catalog",\n            "org.apache.spark.sql.delta.catalog.DeltaCatalog")\n    .getOrCreate()\n)\n\npc = PathlingContext.create(spark)\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import au.csiro.pathling.library.PathlingContext\n\nval spark = SparkSession.builder\n  .config("spark.jars.packages", "io.delta:delta-core_2.12:2.2.0")\n  .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")\n  .config("spark.sql.catalog.spark_catalog",\n    "org.apache.spark.sql.delta.catalog.DeltaCatalog")\n  .getOrCreate()\n\nval pc = PathlingContext.create(spark)\n'))),(0,r.kt)(l.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import au.csiro.pathling.library.PathlingContext;\nimport org.apache.spark.sql.SparkSession;\n\nclass MyApp {\n\n    public static void main(String[] args) {\n        SparkSession spark = SparkSession.builder()\n            .config("spark.jars.packages", "io.delta:delta-core_2.12:2.2.0")\n            .config("spark.sql.extensions", \n                    "io.delta.sql.DeltaSparkSessionExtension")\n            .config("spark.sql.catalog.spark_catalog",\n                    "org.apache.spark.sql.delta.catalog.DeltaCatalog")\n            .getOrCreate();\n        PathlingContext pc = PathlingContext.create(spark);\n    }\n}\n')))),(0,r.kt)("h2",{id:"cluster-configuration"},"Cluster configuration"),(0,r.kt)("p",null,"If you are running your own Spark cluster, or using a Docker image (such\nas ",(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/jupyter/all-spark-notebook"},"jupyter/all-spark-notebook"),"),\nyou will need to configure Pathling as a Spark package."),(0,r.kt)("p",null,"You can do this by adding the following to your ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"spark.jars.packages au.csiro.pathling:library-runtime:[some version]\n")),(0,r.kt)("p",null,"See the ",(0,r.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/configuration.html"},"Configuration"),"\npage of the Spark documentation for more information about ",(0,r.kt)("inlineCode",{parentName:"p"},"spark.jars.packages"),"\nand other related configuration options."),(0,r.kt)("p",null,"To create a Pathling notebook Docker image, your ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," might look like\nthis:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-dockerfile"},'FROM jupyter/all-spark-notebook\n\nUSER root\nRUN echo "spark.jars.packages au.csiro.pathling:library-runtime:[some version]" >> /usr/local/spark/conf/spark-defaults.conf\n\nUSER ${NB_UID}\n\nRUN pip install --quiet --no-cache-dir pathling && \\\n    fix-permissions "${CONDA_DIR}" && \\\n    fix-permissions "/home/${NB_USER}"\n')))}m.isMDXComponent=!0}}]);